{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zixuanhu/anaconda3/envs/tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/zixuanhu/anaconda3/envs/tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/zixuanhu/anaconda3/envs/tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/zixuanhu/anaconda3/envs/tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/zixuanhu/anaconda3/envs/tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/zixuanhu/anaconda3/envs/tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "from planet import tools\n",
    "import functools\n",
    "import os\n",
    "import sys\n",
    "from scipy import stats\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns\n",
    "import multiprocessing\n",
    "import time\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x=np.load('benchmark/finger_spin/baseline3/001/onrival.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1023, 10, 1000, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the trajs of  aug7\n",
      "(5250, 1000, 2)\n"
     ]
    }
   ],
   "source": [
    "# methods={'baseline3': '001' , 'resample_traj4': '002'}\n",
    "methods={ 'aug7': '003'}\n",
    "# methods={'baseline3': '005' , 'resample_traj4': '003'}\n",
    "# methods={'baseline3': '005' , 'aug9': '001'}\n",
    "env = 'finger_spin'\n",
    "# env = 'cheetah_run'\n",
    "# env = 'reacher_easy'\n",
    "PALETTE = 10 * (\n",
    "    '#377eb8', '#4daf4a', '#984ea3', '#e41a1c', '#ff7f00', '#a65628',\n",
    "    '#f781bf', '#888888', '#a6cee3', '#b2df8a', '#cab2d6', '#fb9a99',\n",
    "    '#fdbf6f')\n",
    "buffers = OrderedDict()\n",
    "for k,v in methods.items():\n",
    "    print('Load the trajs of ', k)\n",
    "    buffers[k] = np.load(os.path.join('benchmark', env, k,v,'cem_traj.npy'))\n",
    "    print(buffers[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = 'finger_spin'\n",
    "base = 'baseline3'\n",
    "load_p = 'benchmark/{}/{}/00{}/cem_traj.npy'\n",
    "# buffers[base] = np.concatenate([np.load(load_p.format(env, base, i+1)) for i in range(5)])\n",
    "# buffers['aug7'] = np.concatenate([np.load(load_p.format(env, 'aug7', i+1)) for i in range(5)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'baseline3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-2aa55e402f3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbuffers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'baseline3'"
     ]
    }
   ],
   "source": [
    "buffers[base].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3] [[False  True  True  True  True  True  True  True  True]\n",
      " [False False  True  True  True  True  True  True  True]\n",
      " [False False False  True  True  True  True  True  True]\n",
      " [False False False False  True  True  True  True  True]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8, 11, 12, 13, 14, 15, 16, 17, 21, 22,\n",
       "       23, 24, 25, 26, 31, 32, 33, 34, 35])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def upper_tri_masking(A):\n",
    "    m,n = A.shape[:2]\n",
    "    r = np.arange(m)\n",
    "    c = np.arange(n)\n",
    "    mask = r[:, None] < c\n",
    "    print(r, mask)\n",
    "    return A[mask]\n",
    "x=np.arange(36)\n",
    "x=x.reshape(4,9)\n",
    "upper_tri_masking(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_tri_masking(A):\n",
    "    m = A.shape[0]\n",
    "    r = np.arange(m)\n",
    "    mask = r[:, None] < r\n",
    "    return A[mask]\n",
    "\n",
    "def stratify_by_iter(trajs, part_s=3):\n",
    "    num_parts = int(np.ceil(10 / part_s))\n",
    "    num_iters = trajs.shape[0]\n",
    "    itr = np.arange(num_iters) % 10\n",
    "    batch = {}\n",
    "    prev = np.zeros_like(itr)\n",
    "    for i in range(num_parts):\n",
    "        select = (itr < (i + 1) * part_s).astype(int)\n",
    "        pure_select = select - prev\n",
    "        batch['{}'.format(i * part_s)] = trajs[pure_select.astype(bool)]\n",
    "        prev = select\n",
    "    return batch\n",
    "\n",
    "def horizon_sum(input, horizon=12):\n",
    "    partial_input = input[:, :, :horizon, :]\n",
    "    horizon_sum = np.sum(partial_input, axis=2)\n",
    "    return horizon_sum\n",
    "\n",
    "def data_stats(data, name):\n",
    "#     print('#######################')\n",
    "#     print(name)\n",
    "#     print('Min: {}    Max: {}    Mean: {}    Std: {}'.format(\n",
    "#         data.min(), data.max(), np.nanmean(data), np.nanstd(data)))\n",
    "    return [np.nanmean(data), np.nanstd(data)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def proposals_diagnostic(trajs, pref=None, plot=False):\n",
    "    hor_trajs = horizon_sum(trajs, 12)  # N*1000*2\n",
    "    hor_gts = hor_trajs[:, :, 0]\n",
    "    hor_preds = hor_trajs[:, :, 1]\n",
    "    k = 100\n",
    "    acc_topk, score_gt, score_pred, mean_ratio, rela_acc, cstr_acc, rank_loss = [0.0] * 7\n",
    "    corres_pred_ranks = []\n",
    "    print(trajs.shape, hor_trajs.shape)\n",
    "\n",
    "    num = trajs.shape[0]\n",
    "    for i, (gt, pred) in enumerate(zip(hor_gts, hor_preds)):\n",
    "\n",
    "        gt_ind = np.argsort(gt)\n",
    "        pred_ind = np.argsort(pred)\n",
    "        top_gt_ind = gt_ind[-k:]\n",
    "        top_pred_ind = pred_ind[-k:]\n",
    "        acc_topk += np.intersect1d(top_gt_ind, top_pred_ind).shape[0] / num  # top 100 indices\n",
    "\n",
    "        score_gt += gt[top_gt_ind].mean() / num\n",
    "        score_pred += gt[top_pred_ind].mean() / num\n",
    "        mean_ratio += score_gt / score_pred / num\n",
    "\n",
    "        gt, pred = gt[gt_ind], pred[gt_ind]\n",
    "        gt, pred = gt.reshape(-1, 1), pred.reshape(-1, 1)\n",
    "        gt_diff = gt - gt.T\n",
    "        pred_diff = pred - pred.T\n",
    "        geq = gt_diff > 0\n",
    "#         tgt_posi_dif = np.where(geq, gt_diff, -gt_diff)\n",
    "#         pred_posi_dif = np.where(geq, pred_diff, -pred_diff)\n",
    "        tgt_posi_dif = upper_tri_masking(np.where(geq, gt_diff, -gt_diff))\n",
    "        pred_posi_dif = upper_tri_masking(np.where(geq, pred_diff, -pred_diff))\n",
    "\n",
    "#         rela_acc += np.mean((tgt_posi_dif > 0) == (pred_posi_dif > 0)) / num\n",
    "        rela_acc += old_rank( gt, pred)/num\n",
    "        cstr_acc += np.mean(tgt_posi_dif - pred_posi_dif < 0.0) * 100 / num\n",
    "        rank_loss += np.maximum(0.0, tgt_posi_dif - pred_posi_dif).mean() / num\n",
    "        # error = np.abs(gt_diff<0).sum()\n",
    "\n",
    "        # print(error)\n",
    "        # assert(error==0)\n",
    "\n",
    "        pred_rank = np.zeros_like(pred_ind)\n",
    "        pred_rank[pred_ind] = np.arange(pred_ind.shape[0])\n",
    "        corres_pred_ranks.append(np.array([pred_rank[j] for j in gt_ind]))\n",
    "#         if i < 2 and plot:\n",
    "#             plt.scatter(np.arange(corres_pred_ranks[-1].shape[0]), corres_pred_ranks[-1])\n",
    "#             plt.savefig(os.path.join(OUT_DIR, pref + '_sample%d_rank.png' % i))\n",
    "#             plt.cla()\n",
    "\n",
    "    corres_pred_rank = np.stack(corres_pred_ranks).mean(axis=0)\n",
    "\n",
    "    results = {'acc_topk': acc_topk, 'score_gt': score_gt, 'score_pred': score_pred, 'mean_ratio': mean_ratio,\n",
    "               'rela_acc': rela_acc, 'cstr_acc': cstr_acc, 'rank_loss': rank_loss,\n",
    "               # 'corres_pred_rank': corres_pred_rank,\n",
    "               'traj_return_gt': data_stats(hor_gts, ' Ground truth reward'),\n",
    "               'traj_return_pred': data_stats(hor_preds, ' Predicted reward')}\n",
    "    #print(results['traj_return_gd'],results['traj_return_pred'])\n",
    "    print(results)\n",
    "    return results\n",
    "\n",
    "\n",
    "def planning_diagnostic(buffer):\n",
    "    # (N, 1000, 12, 2)\n",
    "    traj_list = stratify_by_iter(buffer, 10)\n",
    "    results = tools.nested.map(proposals_diagnostic, traj_list)\n",
    "\n",
    "    merged = {\n",
    "              k: np.array([v[k] for v in results.values()])\n",
    "              for k in results['0'].keys()\n",
    "              }\n",
    "    print(merged)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['acc_topk', 'avgRank_topk_by_pred', 'avgRank_topk_by_gt','score_gt', 'score_pred', 'mean_ratio', 'rela_acc_all', 'cstr_acc_all', 'rank_loss_all', \n",
    "           'rela_acc_lowm', 'cstr_acc_lowm', 'rank_loss_lowm','rela_acc_midm', 'cstr_acc_midm', 'rank_loss_midm', 'rela_acc_highr', 'cstr_acc_highr', 'rank_loss_highr',\n",
    "          'rela_acc_lowr', 'cstr_acc_lowr', 'rank_loss_lowr','traj_return_gt','traj_return_pred']\n",
    "def ranking_full(gt_sort_ind, pred_sort_ind):\n",
    "    counter=0\n",
    "    within_gd=np.zeros_like(gt_sort_ind)\n",
    "    within_pred=np.zeros_like(gt_sort_ind)\n",
    "    l = gt_sort_ind.shape[0]\n",
    "    results = []\n",
    "    for i, (gd, pred) in enumerate(zip(gt_sort_ind, pred_sort_ind)):\n",
    "        if gd==pred:\n",
    "            counter+=1\n",
    "        else:\n",
    "            if within_pred[gd]==1:\n",
    "                counter+=1\n",
    "            if within_gd[pred]==1:\n",
    "                counter+=1\n",
    "        within_gd[gd]=1\n",
    "        within_pred[pred]=1\n",
    "        results.append(counter/(i+1)*100)\n",
    "    return np.array(results)\n",
    "\n",
    "def get_rela_acc(gt1, pred1, gt2=None, pred2=None, margin_stra=False):\n",
    "    gt1, pred1 = gt1.reshape(-1, 1), pred1.reshape(-1, 1)\n",
    "    gt_diff = gt1 - gt1.T if gt2 is None else gt1 - gt2.reshape(1, -1)\n",
    "    pred_diff = pred1 - pred1.T if pred2 is None else pred1- pred2.reshape(1, -1)\n",
    "    geq = gt_diff > 0\n",
    "    tgt_posi_dif = np.where(geq, gt_diff, -gt_diff)\n",
    "    pred_posi_dif = np.where(geq, pred_diff, -pred_diff)\n",
    "    \n",
    "#     if margin_stra:\n",
    "#         tgt_posi_dif = upper_tri_masking(tgt_posi_dif)\n",
    "#         pred_posi_dif = upper_tri_masking(pred_posi_dif)\n",
    "        \n",
    "    def do_compute(slt_tgt_diff, slt_pred_diff):\n",
    "        rela_acc = np.mean(slt_pred_diff>0)*100\n",
    "        cstr_acc = np.mean(slt_tgt_diff - slt_pred_diff < 0.0) * 100 \n",
    "        rank_loss = np.maximum(0.0, slt_tgt_diff - slt_pred_diff).mean()\n",
    "        return rela_acc, cstr_acc, rank_loss\n",
    "    \n",
    "    rela_acc_all, cstr_acc_all, rank_loss_all = do_compute(tgt_posi_dif, pred_posi_dif)\n",
    "    if not margin_stra:\n",
    "        return rela_acc_all, cstr_acc_all, rank_loss_all\n",
    "    tgt_posi_dif, pred_posi_dif = tgt_posi_dif.reshape(-1), pred_posi_dif.reshape(-1)\n",
    "    gt_ind = np.argsort(tgt_posi_dif)\n",
    "    tgt_posi_dif, pred_posi_dif = tgt_posi_dif[gt_ind], pred_posi_dif[gt_ind]\n",
    "    \n",
    "    num = tgt_posi_dif.shape[0]\n",
    "    rela_acc_lowm, cstr_acc_lowm, rank_loss_lowm = do_compute(tgt_posi_dif[:num//10], pred_posi_dif[:num//10])\n",
    "#     rela_acc_midm, cstr_acc_midm, rank_loss_midm = do_compute(tgt_posi_dif[:num//5], pred_posi_dif[:num//5])\n",
    "    rela_acc_midm, cstr_acc_midm, rank_loss_midm = do_compute(tgt_posi_dif[:num//2], pred_posi_dif[:num//2])\n",
    "    \n",
    "    return rela_acc_all, cstr_acc_all, rank_loss_all, rela_acc_lowm, cstr_acc_lowm, rank_loss_lowm,rela_acc_midm, cstr_acc_midm, rank_loss_midm\n",
    "    \n",
    "\n",
    "def ana(gt1, pred1):\n",
    "    k=100\n",
    "    \n",
    "    # descending\n",
    "    gt_ind = np.argsort(gt1)[::-1]\n",
    "    pred_ind = np.argsort(pred1)[::-1]\n",
    "    \n",
    "    # Overlapping of top 100 indices\n",
    "    top_gt_ind = gt_ind[:k]\n",
    "    top_pred_ind = pred_ind[:k]\n",
    "    acc_topk = np.intersect1d(top_gt_ind, top_pred_ind).shape[0]  \n",
    "    \n",
    "    # trajectories ranking by ground truth return and predicted return\n",
    "    pred_rank, gt_rank = np.zeros_like(pred_ind), np.zeros_like(pred_ind)\n",
    "    pred_rank[pred_ind], gt_rank[gt_ind] = np.arange(pred_ind.shape[0]), np.arange(pred_ind.shape[0])\n",
    "    avgRank_topk_by_pred = pred_rank[top_gt_ind].mean()\n",
    "    avgRank_topk_by_gt = gt_rank[top_pred_ind].mean()\n",
    "    \n",
    "    acc_rank_full = ranking_full(gt_ind, pred_ind)\n",
    "    assert np.abs(acc_rank_full[k-1]-acc_topk)<1e-5, 'wrong  {}, {}'.format(acc_rank_full[k-1], acc_topk)\n",
    "\n",
    "    score_gt = gt1[top_gt_ind].mean() \n",
    "    score_pred = gt1[top_pred_ind].mean() \n",
    "    mean_ratio = 1.0 \n",
    "       \n",
    "    gt1 = gt1[gt_ind]\n",
    "    pred1 = pred1[gt_ind]\n",
    "    \n",
    "    rela_acc_all, cstr_acc_all, rank_loss_all, rela_acc_lowm, cstr_acc_lowm, rank_loss_lowm,rela_acc_midm, cstr_acc_midm, rank_loss_midm = get_rela_acc(gt1, pred1, margin_stra=True)\n",
    "    rela_acc_highr, cstr_acc_highr, rank_loss_highr = get_rela_acc(gt1[:100], pred1[:100], gt1[:], pred1[:])\n",
    "    rela_acc_lowr, cstr_acc_lowr, rank_loss_lowr = get_rela_acc(gt1[:500], pred1[:500], gt1[:], pred1[:])\n",
    "    \n",
    "    traj_return_gt = gt1.mean()\n",
    "    traj_return_pred = pred1.mean()\n",
    "\n",
    "    return np.array([acc_topk, avgRank_topk_by_pred, avgRank_topk_by_gt, score_gt, score_pred, mean_ratio, rela_acc_all, cstr_acc_all, rank_loss_all, rela_acc_lowm, cstr_acc_lowm, rank_loss_lowm,rela_acc_midm, cstr_acc_midm, rank_loss_midm,\n",
    "                     rela_acc_highr, cstr_acc_highr, rank_loss_highr, rela_acc_lowr, cstr_acc_lowr, rank_loss_lowr, traj_return_gt, traj_return_pred]), acc_rank_full\n",
    "    \n",
    "    \n",
    "def proposals_diagnostic2(trajs, pref=None, multi=True):\n",
    "#     trajs= horizon_sum(trajs)\n",
    "    hor_gts = trajs[:,:,0]\n",
    "    hor_preds = trajs[:,:,1]\n",
    "    k = 100\n",
    "    batch_data = []\n",
    "#     print(trajs.shape)\n",
    "    for hor_gt, hor_pred in zip(hor_gts, hor_preds):\n",
    "        batch_data.append((hor_gt, hor_pred))\n",
    "    start_time = time.time()\n",
    "    if multi:\n",
    "        pool = multiprocessing.Pool(processes=16)\n",
    "        bundle = pool.starmap(ana, batch_data)\n",
    "        pool.close()\n",
    "    else:\n",
    "        bundle=[]\n",
    "        for i,(hor_gt, hor_pred) in enumerate(batch_data):\n",
    "            bundle.append(ana(hor_gt, hor_pred))\n",
    "            \n",
    "    results, full_rank_acc = zip(*bundle)\n",
    "    \n",
    "    df = pd.DataFrame(results, columns=metrics)\n",
    "    \n",
    "    means = np.stack(results, axis=0).mean(0)\n",
    "    full_rank_acc = np.stack(full_rank_acc, axis=0).mean(0)\n",
    "    print('finish')\n",
    "    final = {k:v for k,v in zip(metrics, means)}\n",
    "    final['df'] = df\n",
    "    final['full_rank_acc'] = full_rank_acc\n",
    "    return final\n",
    "\n",
    "\n",
    "    \n",
    "    return batch\n",
    "def planning_diagnostic2(buffer, s=1, multi=True):\n",
    "    # (N, 1000, 12, 2)\n",
    "    traj_list = stratify_by_iter(buffer, s)\n",
    "    func = functools.partial(proposals_diagnostic2, multi=multi)\n",
    "    results = tools.nested.map(func, traj_list)\n",
    "    merged = {\n",
    "#               k: np.array([v[k] for v in results.values()])\n",
    "              k: [v[k] for v in results.values()]\n",
    "              for k in results['0'].keys()\n",
    "              }\n",
    "    for i, df in enumerate(merged['df']):\n",
    "        df['iteration'] = i\n",
    "    merged['df'] = pd.concat(merged['df'])\n",
    "#     print(merged)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n",
      "finish\n",
      "finish\n",
      "finish\n",
      "finish\n",
      "finish\n",
      "finish\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "result = OrderedDict()\n",
    "for method, buffer in buffers.items():\n",
    "    result[method] = planning_diagnostic2(buffer[:20], 1, multi=False)\n",
    "    result[method]['df']['method'] = method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_std(results, metric,hor, show=True, col_st=0):\n",
    "    type = 'hor{}_'.format(hor) + metric\n",
    "    for i, (name, v) in enumerate(results.items()):\n",
    "        mean = v[metric][:, 0]\n",
    "        std = v[metric][:, 1]\n",
    "        xs = np.arange(mean.shape[0])\n",
    "        kw = dict(color=PALETTE[i], alpha=0.1, linewidths=0)\n",
    "        plt.fill_between(xs, mean - std, mean + std, **kw)\n",
    "        plt.plot(xs, mean, color=PALETTE[i+col_st], label=name+'_'+metric)\n",
    "        plt.scatter(xs, mean, color=PALETTE[-i-col_st])\n",
    "    plt.title(type)\n",
    "    plt.legend()\n",
    "#     plt.savefig(os.path.join(OUT_DIR, type+ '.png'))\n",
    "    if show:\n",
    "        plt.show()\n",
    "#     plt.clf()\n",
    "\n",
    "\n",
    "def plot_line(results, metric, hor, show=True, col_st=0):\n",
    "    for i, (name, v) in enumerate(results.items()):\n",
    "        ys = v[metric]\n",
    "        xs = np.arange(len(ys))\n",
    "        plt.plot(xs, ys, color=PALETTE[i+col_st], label=name+'_'+metric)\n",
    "        plt.scatter(xs, ys, color=PALETTE[-i-col_st])\n",
    "    plt.title(type)\n",
    "    plt.legend()\n",
    "    if show:\n",
    "        plt.show()\n",
    "#     plt.savefig(os.path.join(OUT_DIR, type + '.png'))\n",
    "#     plt.clf()\n",
    "def plot_full_ranking(results):\n",
    "    row,col = 2, 5\n",
    "    figsize = (4*col, 4*row)\n",
    "    fig, axs = plt.subplots(row, col, figsize=figsize)\n",
    "    print('plot_full_ranking')\n",
    "    for r in range(row):\n",
    "        for c in range(col):\n",
    "            msg = ''\n",
    "            for i, (k, v) in enumerate(results.items()):\n",
    "                itr=r*col+c\n",
    "                ys = v['full_rank_acc'][itr]\n",
    "                msg+='{}: {}  '.format(k, ys.mean())\n",
    "                xs = np.arange(len(ys))\n",
    "                axs[r,c].plot(xs, ys, color=PALETTE[i], label=k ) \n",
    "                axs[r,c].set_title('CEM iteration {}'.format(itr))\n",
    "            print(msg)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "metrics = ['acc_topk', 'avgRank_topk_by_pred', 'avgRank_topk_by_gt','score_gt', 'score_pred', 'mean_ratio', 'rela_acc_all', 'cstr_acc_all', 'rank_loss_all', \n",
    "           'rela_acc_lowm', 'cstr_acc_lowm', 'rank_loss_lowm','rela_acc_midm', 'cstr_acc_midm', 'rank_loss_midm', 'rela_acc_highr', 'cstr_acc_highr', 'rank_loss_highr',\n",
    "          'rela_acc_lowr', 'cstr_acc_lowr', 'rank_loss_lowr']\n",
    "def plot_all(results, hor):\n",
    "    \n",
    "    metrics = next(iter(results.values())).keys()\n",
    "    df_all = pd.concat([v['df'] for v in results.values()])\n",
    "    sns.set_theme(style=\"darkgrid\")\n",
    "    def lineplot(df, x, y, ci=None,show=True, markers=None, label=None, **kwargs):\n",
    "        sns.lineplot(x=x, y=y, markers=markers, label=label,\n",
    "                 hue=\"method\", data=df_all,ci=ci, **kwargs)\n",
    "        if show:\n",
    "            plt.show()\n",
    "    plot_full_ranking(results)\n",
    "    \n",
    "#     plot_line(results, 'acc_topk', hor)\n",
    "#     plot_line(results, 'avgRank_topk_by_pred', hor)\n",
    "#     plot_line(results, 'avgRank_topk_by_gt', hor)\n",
    "    lineplot(df_all, 'iteration', 'acc_topk', ci='sd')\n",
    "    lineplot(df_all, 'iteration', 'avgRank_topk_by_pred',ci='sd')\n",
    "    lineplot(df_all, 'iteration', 'avgRank_topk_by_gt',ci='sd')\n",
    "    \n",
    "    plot_line(results, 'score_gt', hor, show=False)\n",
    "    plot_line(results, 'score_pred', hor, show=True, col_st=2)\n",
    "    \n",
    "    plot_line(results, 'rela_acc_all', hor, show=False)\n",
    "    plot_line(results, 'rela_acc_midm', hor, show=False, col_st=2)\n",
    "    plot_line(results, 'rela_acc_lowm', hor, col_st=4)\n",
    "    \n",
    "    plot_line(results, 'cstr_acc_all', hor, show=False)\n",
    "    plot_line(results, 'cstr_acc_midm', hor, show=False, col_st=2)\n",
    "    plot_line(results, 'cstr_acc_lowm', hor, col_st=4)\n",
    "    \n",
    "    plot_line(results, 'rank_loss_all', hor, show=False)\n",
    "    plot_line(results, 'rank_loss_midm', hor, show=False, col_st=2)\n",
    "    plot_line(results, 'rank_loss_lowm', hor, col_st=4)\n",
    "    \n",
    "    plot_line(results, 'rela_acc_all', hor, show=False)\n",
    "    plot_line(results, 'rela_acc_highr', hor, show=False, col_st=2)\n",
    "    plot_line(results, 'rela_acc_lowr', hor, col_st=4)\n",
    "    \n",
    "    plot_line(results, 'cstr_acc_all', hor, show=False)\n",
    "    plot_line(results, 'cstr_acc_highr', hor, show=False, col_st=2)\n",
    "    plot_line(results, 'cstr_acc_lowr', hor, col_st=4)\n",
    "    \n",
    "    plot_line(results, 'rank_loss_all', hor, show=False)\n",
    "    plot_line(results, 'rank_loss_highr', hor, show=False, col_st=2)\n",
    "    plot_line(results, 'rank_loss_lowr', hor, col_st=4)\n",
    "    \n",
    "    plot_std(results, 'traj_return_gt', hor, show=False)\n",
    "    plot_std(results, 'traj_return_pred', hor, show=False, col_st=2)\n",
    "    \n",
    "    \n",
    "#     for metric in metrics:\n",
    "#         print('Plot ', metric)\n",
    "#         if 'traj' not in metric:\n",
    "#             plot_line(results, metric, hor)\n",
    "#         else:\n",
    "#             plot_std(results, metric, hor)\n",
    "\n",
    "#     plot_std(results, metric, hor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(result, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, base, rival):\n",
    "    load_p = 'benchmark/{}/{}/00{}/cem_traj.npy'\n",
    "    buffers[base] = np.concatenate([np.load(load_p.format(env, base, i+1)) for i in range(5)])\n",
    "    buffers['aug7'] = np.concatenate([np.load(load_p.format(env, rival, i+1)) for i in range(5)])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4840, 1000, 2)\n"
     ]
    }
   ],
   "source": [
    "x=np.load('benchmark/finger_spin/baseline3/001/cem_traj.npy')\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1971, 10, 1000, 3)\n"
     ]
    }
   ],
   "source": [
    "y=np.load('benchmark/finger_spin/aug7/001/onrival.npy')\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape\n",
    "y2= y.reshape(-1, 1000,3)[:4840, :,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2.shape==x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32177158.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.abs(x-y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in x:\n",
    "    find= False\n",
    "    for i in range(4840):\n",
    "        err = np.sum(np.abs(y2[i]-p))\n",
    "        if err<1e-6:\n",
    "            find=True\n",
    "            break\n",
    "    if not find:\n",
    "        print('fk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1",
   "language": "python",
   "name": "tf1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
